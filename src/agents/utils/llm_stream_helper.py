"""
LLMæµå¼è°ƒç”¨åŠ©æ‰‹
æä¾›ç»Ÿä¸€çš„æµå¼è°ƒç”¨æ¥å£ï¼Œæ”¯æŒå®æ—¶å±•ç¤ºAIå“åº”
"""

from typing import List, Dict, Iterator, Optional
from src.agents.base.base_llm import BaseLLM


class LLMStreamHelper:
    """LLMæµå¼è°ƒç”¨åŠ©æ‰‹"""
    
    @staticmethod
    def think(llm: BaseLLM, messages: List[Dict[str, str]], temperature: float = 0) -> str:
        """
        è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€è€ƒï¼Œå¹¶è¿”å›å…¶å“åº”ã€‚
        æ”¯æŒæµå¼è¾“å‡ºå±•ç¤ºæ•ˆæœã€‚
        
        Args:
            llm: LLMå®ä¾‹
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            å®Œæ•´çš„å“åº”å†…å®¹
        """
        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {type(llm).__name__} æ¨¡å‹...")
        
        # å‡†å¤‡è°ƒç”¨å‚æ•°
        kwargs = {}
        if temperature != 0:
            kwargs['temperature'] = temperature
        
        try:
            # ä¼˜å…ˆä½¿ç”¨æµå¼è°ƒç”¨
            if hasattr(llm, 'stream_invoke'):
                print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:")
                collected_content = []
                
                for chunk in llm.stream_invoke(messages, **kwargs):
                    print(chunk, end="", flush=True)
                    collected_content.append(chunk)
                
                print()  # åœ¨æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ
                return "".join(collected_content)
            else:
                # å›é€€åˆ°æ™®é€šè°ƒç”¨
                response = llm.invoke(messages, **kwargs)
                print(f"âœ… LLMè°ƒç”¨æˆåŠŸ: {response}")
                return response
                
        except Exception as e:
            print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None
    
    @staticmethod
    def stream_think(llm: BaseLLM, messages: List[Dict[str, str]], temperature: float = 0) -> Iterator[str]:
        """
        æµå¼è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ï¼Œè¿”å›ç”Ÿæˆå™¨ã€‚
        
        Args:
            llm: LLMå®ä¾‹
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            å“åº”å†…å®¹çš„ç”Ÿæˆå™¨
        """
        # å‡†å¤‡è°ƒç”¨å‚æ•°
        kwargs = {}
        if temperature != 0:
            kwargs['temperature'] = temperature
        
        try:
            if hasattr(llm, 'stream_invoke'):
                yield from llm.stream_invoke(messages, **kwargs)
            else:
                # å¦‚æœLLMä¸æ”¯æŒæµå¼è°ƒç”¨ï¼Œå°†å®Œæ•´å“åº”ä½œä¸ºå•ä¸ªå—è¿”å›
                response = llm.invoke(messages, **kwargs)
                yield response
        except Exception as e:
            yield f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}"


# ä¾¿æ·å‡½æ•°
def think_with_llm(llm: BaseLLM, messages: List[Dict[str, str]], temperature: float = 0) -> str:
    """ä¾¿æ·å‡½æ•°ï¼šä½¿ç”¨LLMè¿›è¡Œæ€è€ƒ"""
    return LLMStreamHelper.think(llm, messages, temperature)


def stream_think_with_llm(llm: BaseLLM, messages: List[Dict[str, str]], temperature: float = 0) -> Iterator[str]:
    """ä¾¿æ·å‡½æ•°ï¼šæµå¼è°ƒç”¨LLMè¿›è¡Œæ€è€ƒ"""
    return LLMStreamHelper.stream_think(llm, messages, temperature)